{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPYj1eSl32jRrlb6TV/S5A0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sepfutvudIhG","executionInfo":{"status":"ok","timestamp":1701254542513,"user_tz":-60,"elapsed":20591,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}},"outputId":"bceff804-2c7e-4df5-edcd-668e817c298e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"leyRbl8DaYQ6","executionInfo":{"status":"ok","timestamp":1701254549745,"user_tz":-60,"elapsed":7243,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}},"outputId":"17bd43d5-2d50-45aa-8ac6-f526a119ff59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wavenet_vocoder\n","  Downloading wavenet_vocoder-0.1.1.tar.gz (13 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (1.11.3)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from wavenet_vocoder) (2.1.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->wavenet_vocoder) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->wavenet_vocoder) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->wavenet_vocoder) (1.3.0)\n","Building wheels for collected packages: wavenet_vocoder\n","  Building wheel for wavenet_vocoder (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wavenet_vocoder: filename=wavenet_vocoder-0.1.1-py3-none-any.whl size=12647 sha256=9d54c1db1d5c25f71500f35cf9c3dcdb4972f497ea98d9f39522da74d16c5ad5\n","  Stored in directory: /root/.cache/pip/wheels/4f/a4/7b/f1d21f96be36a13e9c3948e8c28792bf8962da19781abd9dc8\n","Successfully built wavenet_vocoder\n","Installing collected packages: wavenet_vocoder\n","Successfully installed wavenet_vocoder-0.1.1\n"]}],"source":["%pip install wavenet_vocoder"]},{"cell_type":"code","source":["%pip install webrtcvad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKRoUiy9obaf","executionInfo":{"status":"ok","timestamp":1701254561620,"user_tz":-60,"elapsed":11904,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}},"outputId":"c87faa35-f2da-4301-a75c-d23029755fb8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting webrtcvad\n","  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: webrtcvad\n","  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp310-cp310-linux_x86_64.whl size=73470 sha256=fc570ad603568dbff2649a9fe30e35aa4216b13b63b4f229ce4b1442487a6faa\n","  Stored in directory: /root/.cache/pip/wheels/2a/2b/84/ac7bacfe8c68a87c1ee3dd3c66818a54c71599abf308e8eb35\n","Successfully built webrtcvad\n","Installing collected packages: webrtcvad\n","Successfully installed webrtcvad-2.0.10\n"]}]},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","from wavenet_vocoder import builder\n","\n","import random\n","import struct\n","from pathlib import Path\n","from typing import Optional, Union\n","\n","import librosa\n","import numpy as np\n","import webrtcvad\n","from scipy.ndimage import binary_dilation\n","\n","import soundfile as sf\n","from scipy import signal\n","from scipy.signal import get_window\n","from librosa.filters import mel\n","from numpy.random import RandomState\n","import os\n","import pickle\n","\n"],"metadata":{"id":"z54TFHzQcHhf","executionInfo":{"status":"ok","timestamp":1701254568581,"user_tz":-60,"elapsed":6970,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device type available = '{device}'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9dRlqcUdt9n","executionInfo":{"status":"ok","timestamp":1701254568584,"user_tz":-60,"elapsed":67,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}},"outputId":"7e6222c4-a9f9-4d02-9f41-5f327b940be5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Device type available = 'cuda:0'\n"]}]},{"cell_type":"code","source":["class DictWithDotNotation(dict):\n","    \"\"\"\n","    a dictionary that supports dot notation\n","    as well as dictionary access notation\n","    usage: d = DotDict() or d = DotDict({'val1':'first'})\n","    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n","    get attributes: d.val2 or d['val2']\n","    \"\"\"\n","    __getattr__ = dict.__getitem__\n","    __setattr__ = dict.__setitem__\n","    __delattr__ = dict.__delitem__\n","\n","    def __init__(self, dct=None):\n","        dct = dict() if not dct else dct\n","        for key, value in dct.items():\n","            if hasattr(value, 'keys'):\n","                value = DictWithDotNotation(value)\n","            self[key] = value\n","\n","\n","class GetDictWithDotNotation(DictWithDotNotation):\n","\n","    def __init__(self, hp_dict):\n","        super(DictWithDotNotation, self).__init__()\n","\n","        hp_dotdict = DictWithDotNotation(hp_dict)\n","        for k, v in hp_dotdict.items():\n","            setattr(self, k, v)\n","\n","    __getattr__ = DictWithDotNotation.__getitem__\n","    __setattr__ = DictWithDotNotation.__setitem__\n","    __delattr__ = DictWithDotNotation.__delitem__"],"metadata":{"id":"QpM_E2X0gzuJ","executionInfo":{"status":"ok","timestamp":1701254568586,"user_tz":-60,"elapsed":50,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["PROJECT_DIR = \"\"\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(f\"Device type available = '{device}'\")\n","\n","hparam_dict = {\n","\n","    # genereal parameters\n","    \"general\": {\n","        # small error\n","        \"small_err\": 1e-6,\n","\n","        \"is_training_mode\": True,\n","        \"device\": device,\n","        \"project_root\": PROJECT_DIR,\n","\n","    },\n","\n","    # path to the raw audio file\n","    \"raw_audio\": {\n","        \"raw_audio_path\": \"static/raw_data/wavs\",\n","        \"train_spectrogram_path\": \"static/spectrograms/train\",\n","        \"test_spectrogram_path\": \"static/spectrograms/test\",\n","        \"train_percent\": .8,\n","    },\n","\n","    ## Audio\n","    # same audio settings to be used in the wavenet model to reconstruct the audio from mel-spectrogram\n","    \"audio\": {\n","        \"sampling_rate\": 16000,\n","        # Number of spectrogram frames in a partial utterance\n","        \"partials_n_frames\": 180,  # 1600 ms\n","\n","        \"n_fft\": 1024,  # 1024 seems to work well\n","        \"hop_length\": 1024 // 4,  # n_fft/4 seems to work better\n","\n","        \"mel_window_length\": 25,  # In milliseconds\n","        \"mel_window_step\": 10,  # In milliseconds\n","        \"mel_n_channels\": 80,\n","\n","    },\n","\n","    ## Voice Activation Detection\n","    \"vad\": {\n","        # Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n","        # This sets the granularity of the VAD. Should not need to be changed.\n","        \"vad_window_length\": 30,  # In milliseconds\n","        # Number of frames to average together when performing the moving average smoothing.\n","        # The larger this value, the larger the VAD variations must be to not get smoothed out.\n","        \"vad_moving_average_width\": 8,\n","        # Maximum number of consecutive silent frames a segment can have.\n","        \"vad_max_silence_length\": 6,\n","\n","        ## Audio volume normalization\n","        \"audio_norm_target_dBFS\": -30,\n","        \"rate_partial_slices\": 1.3,\n","        \"min_coverage\": 0.75,\n","    },\n","\n","    \"m_wave_net\": {\n","        \"gen\": {\n","            \"best_model_path\": \"static/model_chk_pts/wavenet_model/checkpoint_step001000000_ema.pth\"\n","        },\n","        \"hp\": {\n","            # DO NOT CHANGE THESE HP\n","            'name': \"wavenet_vocoder\",\n","\n","            # Convenient model builder\n","            'builder': \"wavenet\",\n","\n","            # Input type:\n","            # 1. raw [-1, 1]\n","            # 2. mulaw [-1, 1]\n","            # 3. mulaw-quantize [0, mu]\n","            # If input_type is raw or mulaw, network assumes scalar input and\n","            # discretized mixture of logistic distributions output, otherwise one-hot\n","            # input and softmax output are assumed.\n","            # **NOTE**: if you change the one of the two parameters below, you need to\n","            # re-run preprocessing before training.\n","            'input_type': \"raw\",\n","            'quantize_channels': 65536,  # 65536 or 256\n","\n","            # Audio: these 4 items to be same as used to create mel out of audio\n","            # 'sample_rate': 16000,\n","            # 'fft_size': 1024,\n","            # # shift can be specified by either hop_size or frame_shift_ms\n","            # 'hop_size': 256,\n","            # 'num_mels': 80,\n","\n","            # this is only valid for mulaw is True\n","            'silence_threshold': 2,\n","\n","            'fmin': 125,\n","            'fmax': 7600,\n","            'frame_shift_ms': None,\n","            'min_level_db': -100,\n","            'ref_level_db': 20,\n","            # whether to rescale waveform or not.\n","            # Let x is an input waveform, rescaled waveform y is given by:\n","            # y = x / np.abs(x).max() * rescaling_max\n","            'rescaling': True,\n","            'rescaling_max': 0.999,\n","            # mel-spectrogram is normalized to [0, 1] for each utterance and clipping may\n","            # happen depends on min_level_db and ref_level_db, causing clipping noise.\n","            # If False, assertion is added to ensure no clipping happens.o0\n","            'allow_clipping_in_normalization': True,\n","\n","            # Mixture of logistic distributions:\n","            'log_scale_min': float(-32.23619130191664),\n","\n","            # Model:\n","            # This should equal to `quantize_channels` if mu-law quantize enabled\n","            # otherwise num_mixture * 3 (pi, mean, log_scale)\n","            'out_channels': 10 * 3,\n","            'layers': 24,\n","            'stacks': 4,\n","            'residual_channels': 512,\n","            'gate_channels': 512,  # split into 2 gropus internally for gated activation\n","            'skip_out_channels': 256,\n","            'dropout': 1 - 0.95,\n","            'kernel_size': 3,\n","            # If True, apply weight normalization as same as DeepVoice3\n","            'weight_normalization': True,\n","            # Use legacy code or not. Default is True since we already provided a model\n","            # based on the legacy code that can generate high-quality audio.\n","            # Ref: https://github.com/r9y9/wavenet_vocoder/pull/73\n","            'legacy': True,\n","\n","            # Local conditioning (set negative value to disable))\n","            'cin_channels': 80,\n","            # If True, use transposed convolutions to upsample conditional features,\n","            # otherwise repeat features to adjust time resolution\n","            'upsample_conditional_features': True,\n","            # should np.prod(upsample_scales) == hop_size\n","            'upsample_scales': [4, 4, 4, 4],\n","            # Freq axis kernel size for upsampling network\n","            'freq_axis_kernel_size': 3,\n","\n","            # Global conditioning (set negative value to disable)\n","            # currently limited for speaker embedding\n","            # this should only be enabled for multi-speaker dataset\n","            'gin_channels': -1,  # i.e., speaker embedding dim\n","            'n_speakers': -1,\n","\n","            # Data loader\n","            'pin_memory': True,\n","            'num_workers': 2,\n","\n","            # train/test\n","            # test size can be specified as portion or num samples\n","            'test_size': 0.0441,  # 50 for CMU ARCTIC single speaker\n","            'test_num_samples': None,\n","            'random_state': 1234,\n","\n","            # Loss\n","\n","            # Training:\n","            'batch_size': 2,\n","            'adam_beta1': 0.9,\n","            'adam_beta2': 0.999,\n","            'adam_eps': 1e-8,\n","            'amsgrad': False,\n","            'initial_learning_rate': 1e-3,\n","            # see lrschedule.py for available lr_schedule\n","            'lr_schedule': \"noam_learning_rate_decay\",\n","            'lr_schedule_kwargs': {},  # {\"anneal_rate\": 0.5, \"anneal_interval\": 50000},\n","            'nepochs': 2000,\n","            'weight_decay': 0.0,\n","            'clip_thresh': -1,\n","            # max time steps can either be specified as sec or steps\n","            # if both are None, then full audio samples are used in a batch\n","            'max_time_sec': None,\n","            'max_time_steps': 8000,\n","            # Hold moving averaged parameters and use them for evaluation\n","            'exponential_moving_average': True,\n","            # averaged = decay * averaged + (1 - decay) * x\n","            'ema_decay': 0.9999,\n","\n","            # Save\n","            # per-step intervals\n","            'checkpoint_interval': 10000,\n","            'train_eval_interval': 10000,\n","            # per-epoch interval\n","            'test_eval_epoch_interval': 5,\n","            'save_optimizer_state': True,\n","\n","            # Eval:\n","        }\n","\n","    }\n","}\n","\n","# this hp will be used throughout the project\n","hp = GetDictWithDotNotation(hparam_dict)\n","\n","# few calculated values from wavenet model\n","hp.m_wave_net.hp.sample_rate = hp.audio.sampling_rate\n","hp.m_wave_net.hp.fft_size = hp.audio.n_fft\n","hp.m_wave_net.hp.hop_size = hp.audio.hop_length\n","hp.m_wave_net.hp.num_mels = hp.audio.mel_n_channels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oHeiLS3Xg02i","executionInfo":{"status":"ok","timestamp":1701254568590,"user_tz":-60,"elapsed":53,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}},"outputId":"9c66c921-ff8e-431e-df4c-b2776bc63aa4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Device type available = 'cuda:0'\n"]}]},{"cell_type":"code","source":["int16_max = (2 ** 15) - 1\n","\n","\n","def preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray], hp, source_sr: Optional[int] = None):\n","    \"\"\"\n","    Applies preprocessing operations to a waveform either on disk or in memory such that\n","    The waveform will be resampled to match the data hyperparameters.\n","\n","    :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not\n","    just .wav), either the waveform as a numpy array of floats.\n","    :param source_sr: if passing an audio waveform, the sampling rate of the waveform before\n","    preprocessing. After preprocessing, the waveform'speaker sampling rate will match the data\n","    hyperparameters. If passing a filepath, the sampling rate will be automatically detected and\n","    this argument will be ignored.\n","    \"\"\"\n","    # Load the wav from disk if needed\n","    if isinstance(fpath_or_wav, str) or isinstance(fpath_or_wav, Path):\n","        wav, source_sr = librosa.load(str(fpath_or_wav), sr=None)\n","    else:\n","        wav = fpath_or_wav\n","\n","    # Resample the wav\n","    if source_sr is not None:\n","        wav = librosa.resample(y=wav, orig_sr=source_sr, target_sr=hp.audio.sampling_rate)\n","\n","    # Apply the preprocessing: normalize volume and shorten long silences\n","    wav = normalize_volume(wav, hp.vad.audio_norm_target_dBFS, increase_only=True)\n","    wav = trim_long_silences(wav, hp)\n","\n","    return wav\n","\n","\n","def trim_long_silences(wav, hp):\n","    \"\"\"\n","    Ensures that segments without voice in the waveform remain no longer than a\n","    threshold determined by the VAD parameters in params.py.\n","\n","    :param wav: the raw waveform as a numpy array of floats\n","    :return: the same waveform with silences trimmed away (length <= original wav length)\n","    \"\"\"\n","    # Compute the voice detection window size\n","    samples_per_window = (hp.vad.vad_window_length * hp.audio.sampling_rate) // 1000\n","\n","    # Trim the end of the audio to have a multiple of the window size\n","    wav = wav[:len(wav) - (len(wav) % samples_per_window)]\n","\n","    # Convert the float waveform to 16-bit mono PCM\n","    pcm_wave = struct.pack(\"%dh\" % len(wav), *(np.round(wav * int16_max)).astype(np.int16))\n","\n","    # Perform voice activation detection\n","    voice_flags = []\n","    vad = webrtcvad.Vad(mode=3)\n","    for window_start in range(0, len(wav), samples_per_window):\n","        window_end = window_start + samples_per_window\n","        voice_flag = vad.is_speech(pcm_wave[window_start * 2:window_end * 2], sample_rate=hp.audio.sampling_rate)\n","        voice_flags.append(voice_flag)\n","\n","    voice_flags = np.array(voice_flags)\n","\n","    # Smooth the voice detection with a moving average\n","    def moving_average(array, width):\n","        array_padded = np.concatenate((np.zeros((width - 1) // 2), array, np.zeros(width // 2)))\n","        ret = np.cumsum(array_padded, dtype=float)\n","        ret[width:] = ret[width:] - ret[:-width]\n","        return ret[width - 1:] / width\n","\n","    audio_mask = moving_average(voice_flags, hp.vad.vad_moving_average_width)\n","    audio_mask = np.round(audio_mask).astype(np.bool)\n","\n","    # Dilate the voiced regions\n","    audio_mask = binary_dilation(audio_mask, np.ones(hp.vad.vad_max_silence_length + 1))\n","    audio_mask = np.repeat(audio_mask, samples_per_window)\n","\n","    return wav[audio_mask == True]\n","\n","\n","def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n","    if increase_only and decrease_only:\n","        raise ValueError(\"Both increase only and decrease only are set\")\n","    rms = np.sqrt(np.mean((wav * int16_max) ** 2))\n","    wave_dBFS = 20 * np.log10(rms / int16_max)\n","    dBFS_change = target_dBFS - wave_dBFS\n","    if dBFS_change < 0 and increase_only or dBFS_change > 0 and decrease_only:\n","        return wav\n","    return wav * (10 ** (dBFS_change / 20))\n","\n","\n","def compute_partial_slices(n_samples: int, hp):\n","    \"\"\"\n","    Computes where to split an utterance waveform and its corresponding mel spectrogram to\n","    obtain partial utterances of <partials_n_frames> each. Both the waveform and the\n","    mel spectrogram slices are returned, so as to make each partial utterance waveform\n","    correspond to its spectrogram.\n","\n","    The returned ranges may be indexing further than the length of the waveform. It is\n","    recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\n","\n","    :param n_samples: the number of samples in the waveform\n","    :param rate: how many partial utterances should occur per second. Partial utterances must\n","    cover the span of the entire utterance, thus the rate should not be lower than the inverse\n","    of the duration of a partial utterance. By default, partial utterances are 1.6s long and\n","    the minimum rate is thus 0.625.\n","    :param min_coverage: when reaching the last partial utterance, it may or may not have\n","    enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\n","    then the last partial utterance will be considered by zero-padding the audio. Otherwise,\n","    it will be discarded. If there aren't enough frames for one partial utterance,\n","    this parameter is ignored so that the function always returns at least one slice.\n","    :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\n","    respectively the waveform and the mel spectrogram with these slices to obtain the partial\n","    utterances.\n","    \"\"\"\n","    assert 0 < hp.vad.min_coverage <= 1\n","\n","    # Compute how many frames separate two partial utterances\n","    samples_per_frame = int((hp.audio.sampling_rate * hp.mel_fb.mel_window_step / 1000))\n","    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n","    frame_step = int(np.round((hp.audio.sampling_rate / hp.vad.rate_partial_slices) / samples_per_frame))\n","\n","    min_frame_step = (hp.audio.sampling_rate / (samples_per_frame * hp.audio.partials_n_frames))\n","    assert 0 < frame_step, \"The rate is too high\"\n","    assert frame_step <= hp.audio.partials_n_frames, \"The rate is too low, it should be %f at least\" % min_frame_step\n","\n","    # Compute the slices\n","    wav_slices, mel_slices = [], []\n","    steps = max(1, n_frames - hp.audio.partials_n_frames + frame_step + 1)\n","    for i in range(0, steps, frame_step):\n","        mel_range = np.array([i, i + hp.audio.partials_n_frames])\n","        wav_range = mel_range * samples_per_frame\n","        mel_slices.append(slice(*mel_range))\n","        wav_slices.append(slice(*wav_range))\n","\n","    # Evaluate whether extra padding is warranted or not\n","    last_wav_range = wav_slices[-1]\n","    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n","    if coverage < hp.vad.min_coverage and len(mel_slices) > 1:\n","        mel_slices = mel_slices[:-1]\n","        wav_slices = wav_slices[:-1]\n","\n","    return wav_slices, mel_slices\n","\n","\n","def pySTFT(x, fft_length=1024, hop_length=256):\n","    \"\"\"\n","    this function returns spectrogram\n","    :param x: np array for the audio file\n","    :param fft_length: fft length for fast fourier transform (https://www.youtube.com/watch?v=E8HeD-MUrjY)\n","    :param hop_length: hop_lenght is the sliding overlapping window size normally fft//4 works the best\n","    :return: spectrogram in the form of np array\n","    \"\"\"\n","    x = np.pad(x, int(fft_length // 2), mode='reflect')\n","\n","    noverlap = fft_length - hop_length\n","    shape = x.shape[:-1] + ((x.shape[-1] - noverlap) // hop_length, fft_length)\n","    strides = x.strides[:-1] + (hop_length * x.strides[-1], x.strides[-1])\n","    result = np.lib.stride_tricks.as_strided(x, shape=shape, strides=strides)\n","\n","    fft_window = get_window('hann', fft_length, fftbins=True)\n","    result = np.fft.rfft(fft_window * result, n=fft_length).T\n","\n","    return np.abs(result)\n","\n","\n","def butter_highpass(cutoff, fs, order=5):\n","    nyq = 0.5 * fs\n","    normal_cutoff = cutoff / nyq\n","    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n","    return b, a\n","\n","\n","def wav_to_mel_spectrogram(wav, hp):\n","    \"\"\"\n","    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.\n","    Note: this not a log-mel spectrogram.\n","    \"\"\"\n","\n","    # creating mel basis matrix\n","    mel_basis = mel(sr=hp.audio.sampling_rate,\n","                    n_fft=hp.audio.n_fft,\n","                    fmin=90,\n","                    fmax=7600,\n","                    n_mels=hp.audio.mel_n_channels).T\n","\n","    min_level = np.exp(-100 / 20 * np.log(10))\n","\n","    # getting audio as a np array\n","    pp_wav = preprocess_wav(wav, hp)\n","\n","    # Compute spect\n","    spectrogram = pySTFT(pp_wav).T\n","    # Convert to mel and normalize\n","    mel_spect = np.dot(spectrogram, mel_basis)\n","    D_db = 20 * np.log10(np.maximum(min_level, mel_spect)) - 16\n","    norm_mel_spect = np.clip((D_db + 100) / 100, 0, 1)\n","\n","    return norm_mel_spect\n","\n","\n","def shuffle_along_axis(a, axis):\n","    \"\"\"\n","    :param a: nd array e.g. [40, 180, 80]\n","    :param axis: array axis. e.g. 0\n","    :return: a shuffled np array along the given axis\n","    \"\"\"\n","    idx = np.random.rand(*a.shape).argsort(axis=axis)\n","    return np.take_along_axis(a, idx, axis=axis)"],"metadata":{"id":"cJy1LH0uhCsr","executionInfo":{"status":"ok","timestamp":1701254568594,"user_tz":-60,"elapsed":37,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["torch.set_num_threads(4)\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","\n","hparams = hp.m_wave_net.hp\n","\n","def build_model():\n","    model = getattr(builder, hparams.builder)(\n","        out_channels=hparams.out_channels,\n","        layers=hparams.layers,\n","        stacks=hparams.stacks,\n","        residual_channels=hparams.residual_channels,\n","        gate_channels=hparams.gate_channels,\n","        skip_out_channels=hparams.skip_out_channels,\n","        cin_channels=hparams.cin_channels,\n","        gin_channels=hparams.gin_channels,\n","        weight_normalization=hparams.weight_normalization,\n","        n_speakers=hparams.n_speakers,\n","        dropout=hparams.dropout,\n","        kernel_size=hparams.kernel_size,\n","        upsample_conditional_features=hparams.upsample_conditional_features,\n","        upsample_scales=hparams.upsample_scales,\n","        freq_axis_kernel_size=hparams.freq_axis_kernel_size,\n","        scalar_input=True,\n","        legacy=hparams.legacy,\n","    )\n","    return model\n","\n","\n","def wavegen(model, c=None, tqdm=tqdm):\n","    \"\"\"Generate waveform samples by WaveNet.\n","\n","    \"\"\"\n","\n","    model.eval()\n","    model.make_generation_fast_()\n","\n","    Tc = c.shape[0]\n","    upsample_factor = hparams.hop_size\n","    # Overwrite length according to feature size\n","    length = Tc * upsample_factor\n","\n","    # B x C x T\n","    c = torch.FloatTensor(c.T).unsqueeze(0)\n","\n","    initial_input = torch.zeros(1, 1, 1).fill_(0.0)\n","\n","    # Transform data to GPU\n","    initial_input = initial_input.to(device)\n","    c = None if c is None else c.to(device)\n","\n","    with torch.no_grad():\n","        y_hat = model.incremental_forward(\n","            initial_input, c=c, g=None, T=length, tqdm=tqdm, softmax=True, quantize=True,\n","            log_scale_min=hparams.log_scale_min)\n","\n","    y_hat = y_hat.view(-1).cpu().data.numpy()\n","\n","    return y_hat\n","\n","model = build_model().to(device)"],"metadata":{"id":"oWC-fQ2rhJml","executionInfo":{"status":"ok","timestamp":1701254577077,"user_tz":-60,"elapsed":8516,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b4971a2-b451-48c5-fcfb-95769e1ee640"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n","  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"]}]},{"cell_type":"code","source":["c = \"./drive/MyDrive/wavenet_models/20180510_mixture_lj_checkpoint_step000320000_ema.pth\"\n","checkpoint = torch.load(c, map_location=device)\n","model.load_state_dict(checkpoint[\"state_dict\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ttPEg076hkrW","executionInfo":{"status":"ok","timestamp":1701254582055,"user_tz":-60,"elapsed":4987,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}},"outputId":"fa5f4e50-f07f-44d3-a63f-7d940f429bc1"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["wav_path = \"./drive/MyDrive/hola.ogg\"\n","mel = wav_to_mel_spectrogram(wav_path, hp)\n","mel.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YVfAiaGCnH9L","executionInfo":{"status":"ok","timestamp":1701254595884,"user_tz":-60,"elapsed":13843,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}},"outputId":"dce860db-6476-4d48-8e3e-3d755bc98523"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-d7909d4657a7>:68: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  audio_mask = np.round(audio_mask).astype(np.bool)\n"]},{"output_type":"execute_result","data":{"text/plain":["(190, 80)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["waveform = wavegen(model, c=mel)\n","sf.write('reconstructed_audio.wav', waveform, 16000, 'PCM_24')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYZbxJIypBXW","executionInfo":{"status":"ok","timestamp":1701255244729,"user_tz":-60,"elapsed":599764,"user":{"displayName":"Pablo González Santamarta","userId":"05322282046654241130"}},"outputId":"185649de-b344-466f-ad6f-9670ac4ce5e0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 48640/48640 [09:59<00:00, 81.16it/s]\n"]}]}]}