{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNaV+TUHBjmJ9wixZuiy/bA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAiZGyoAyDxB","executionInfo":{"status":"ok","timestamp":1698695738806,"user_tz":-60,"elapsed":1999,"user":{"displayName":"Pablo Gonz√°lez Santamarta","userId":"05322282046654241130"}},"outputId":"b1c15d22-a6b2-47fa-d184-155803082bbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import vgg16\n","from collections import namedtuple\n","import requests\n","from tqdm import tqdm\n","import albumentations\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from torchvision import utils as vutils"],"metadata":{"id":"DWC8BbtZAkyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["work_dir = \"./drive/MyDrive/VAE_learning\"\n","\n","if not os.path.isdir(work_dir):\n","  os.makedirs(work_dir)"],"metadata":{"id":"qdevtcvImM0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper Classes\n","# Group Normalization Layer\n","class GroupNorm(nn.Module):\n","  def __init__(self, channels):\n","    super(GroupNorm, self).__init__()\n","    self.gn = nn.GroupNorm(num_groups = 32, num_channels=channels)\n","\n","  def forward(self, x):\n","    return self.gn(x)\n","\n","#Swish Layer\n","class Swish(nn.Module):\n","  def forward(self, x):\n","    return x*torch.sigmoid(x)\n","\n","#ResidualBlock\n","class ResidualBlock(nn.Module):\n","  def __init__(self, in_channels, out_channels):\n","    super(ResidualBlock, self).__init__()\n","    self.in_channels = in_channels\n","    self.out_channels = out_channels\n","    self.block = nn.Sequential(\n","        GroupNorm(in_channels),\n","        Swish(),\n","        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n","        GroupNorm(out_channels),\n","        Swish(),\n","        nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n","        GroupNorm(out_channels),\n","        Swish()\n","    )\n","\n","    if in_channels != out_channels:\n","      self.channel_up = nn.Conv2d(in_channels, out_channels, 1, 1 ,0)\n","\n","  def forward(self, x):\n","    if self.in_channels != self.out_channels:\n","      return self.channel_up(x) + self.block(x)\n","\n","    else:\n","      return x + self.block(x)\n","\n","#UpSample\n","class UpSampleBlock(nn.Module):\n","  def __init__(self, channels):\n","    super(UpSampleBlock, self).__init__()\n","    self.conv = nn.Conv2d(channels, channels, 3, 1 ,1)\n","\n","  def forward(self, x):\n","    x = F.interpolate(x, scale_factor=2.0)\n","    return self.conv(x)\n","\n","#DownSample\n","class DownSampleBlock(nn.Module):\n","  def __init__(self, channels):\n","    super(DownSampleBlock, self).__init__()\n","    self.conv = nn.Conv2d(channels, channels, 3, 2 ,0)\n","\n","  def forward(self, x):\n","    pad = (0, 1, 0, 1)\n","    x = F.pad(x, pad, mode=\"constant\", value=0)\n","    return self.conv(x)\n","\n","#NonLocalBlock\n","class NonLocalBlock(nn.Module):\n","    def __init__(self, channels):\n","        super(NonLocalBlock, self).__init__()\n","        self.in_channels = channels\n","\n","        self.gn = GroupNorm(channels)\n","        self.q = nn.Conv2d(channels, channels, 1, 1, 0)\n","        self.k = nn.Conv2d(channels, channels, 1, 1, 0)\n","        self.v = nn.Conv2d(channels, channels, 1, 1, 0)\n","        self.proj_out = nn.Conv2d(channels, channels, 1, 1, 0)\n","\n","    def forward(self, x):\n","        h_ = self.gn(x)\n","        q = self.q(h_)\n","        k = self.k(h_)\n","        v = self.v(h_)\n","\n","        b, c, h, w = q.shape\n","\n","        q = q.reshape(b, c, h*w)\n","        q = q.permute(0, 2, 1)\n","        k = k.reshape(b, c, h*w)\n","        v = v.reshape(b, c, h*w)\n","\n","        attn = torch.bmm(q, k)\n","        attn = attn * (int(c)**(-0.5))\n","        attn = F.softmax(attn, dim=2)\n","        attn = attn.permute(0, 2, 1)\n","\n","        A = torch.bmm(v, attn)\n","        A = A.reshape(b, c, h, w)\n","\n","        return x + A\n","\n","\n"],"metadata":{"id":"BA6XLTDh9XKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Encoder\n","\n","class Encoder(nn.Module):\n","  def __init__(self, args):\n","    super(Encoder, self).__init__()\n","    channels = [128,128,128,256,256,512]\n","    attn_resolutions = [16]\n","    num_res_blocks=2\n","    resolution = 256\n","    layers=[nn.Conv2d(args.image_channels, channels[0], 3, 1, 1)]\n","    for i in range(len(channels)-1):\n","      in_channels = channels[i]\n","      out_channels = channels[i+1]\n","      for j in range(num_res_blocks):\n","        layers.append(ResidualBlock(in_channels, out_channels))\n","        in_channels = out_channels\n","        if resolution in attn_resolutions:\n","          layers.append(NonLocalBlock(in_channels))\n","      if i != len(channels)-2:\n","        layers.append(DownSampleBlock(channels[i+1]))\n","        resolution //= 2\n","    layers.append(ResidualBlock(channels[-1], channels[-1]))\n","    layers.append(NonLocalBlock(channels[-1]))\n","    layers.append(ResidualBlock(channels[-1], channels[-1]))\n","    layers.append(GroupNorm(channels[-1]))\n","    layers.append(Swish())\n","    layers.append(nn.Conv2d(channels[-1], args.latent_dim, 3, 1, 1))\n","    self.model = nn.Sequential(*layers)\n","\n","  def forward(self, x):\n","    return self.model(x)\n"],"metadata":{"id":"NnlelxrvFB0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Decoder\n","\n","class Decoder(nn.Module):\n","  def __init__(self, args):\n","    super(Decoder, self).__init__()\n","    channels = [512,256,256,128,128]\n","    attn_resolutions=[16]\n","    num_res_blocks = 3\n","    resolution = 16\n","    in_channels = channels[0]\n","\n","    layers = [nn.Conv2d(args.latent_dim, in_channels, 3, 1, 1),\n","              ResidualBlock(in_channels, in_channels),\n","              NonLocalBlock(in_channels),\n","              ResidualBlock(in_channels, in_channels)]\n","\n","    for i in range(len(channels)):\n","      out_channels = channels[i]\n","      for j in range(num_res_blocks):\n","        layers.append(ResidualBlock(in_channels, out_channels))\n","        in_channels = out_channels\n","        if resolution in attn_resolutions:\n","          layers.append(NonLocalBlock(in_channels))\n","      if i != 0:\n","        layers.append(UpSampleBlock(in_channels))\n","        resolution*=2\n","\n","    layers.append(GroupNorm(in_channels))\n","    layers.append(Swish())\n","    layers.append(nn.Conv2d(in_channels, args.image_channels, 3, 1 ,1))\n","    self.model = nn.Sequential(*layers)\n","\n","  def forward(self,x):\n","    return self.model(x)"],"metadata":{"id":"wF2U281eIJ7c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Codebook\n","\n","class Codebook(nn.Module):\n","  def __init__(self, args):\n","    super(Codebook, self).__init__()\n","    self.num_codebook_vectors = args.num_codebook_vectors\n","    self.latent_dim = args.latent_dim\n","    self.beta = args.beta\n","\n","    self.embedding = nn.Embedding(self.num_codebook_vectors, self.latent_dim)\n","    self.embedding.weight.data.uniform_(-1.0 /self.num_codebook_vectors, -1.0 /self.num_codebook_vectors)\n","\n","  def forward(self, z):\n","    z = z.permute(0,2,3,1).contiguous()\n","    z_flattened = z.view(-1, self.latent_dim)\n","\n","    d = torch.sum(z_flattened**2, dim=1, keepdim=True) + \\\n","        torch.sum(self.embedding.weight ** 2, dim=1) - \\\n","        2*torch.matmul(z_flattened, self.embedding.weight.t())\n","\n","    min_encoding_indices = torch.argmin(d, dim=1)\n","    z_q = self.embedding(min_encoding_indices).view(z.shape)\n","\n","    loss = torch.mean((z_q.detach() - z)**2 + self.beta + (z_q - z.detach())**2)\n","\n","    z_q = z -(z_q.detach())\n","\n","    z_q = z_q.permute(0,3,1,2)\n","\n","    return z_q, min_encoding_indices, loss\n","\n","\n"],"metadata":{"id":"6X-I1NM_GNPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#VQGAN\n","\n","class VQGAN(nn.Module):\n","  def __init__(self, args):\n","    super(VQGAN,self).__init__()\n","    self.encoder = Encoder(args).to(device=args.device)\n","    self.decoder = Decoder(args).to(device=args.device)\n","    self.codebook = Codebook(args).to(device=args.device)\n","    self.quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, kernel_size=1).to(device=args.device)\n","    self.post_quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, kernel_size=1).to(device=args.device)\n","\n","  def forward(self, imgs):\n","    encoded_images = self.encoder(imgs)\n","    quant_conv_encoded_images = self.quant_conv(encoded_images)\n","    codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n","    post_quant_conv_mapping = self.post_quant_conv(codebook_mapping)\n","    decoded_images = self.decoder(post_quant_conv_mapping)\n","\n","    return decoded_images, codebook_indices, q_loss\n","\n","  def encode(self, imgs):\n","    encoded_images = self.encoder(imgs)\n","    quant_conv_encoded_images = self.quant_conv(encoded_images)\n","    codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n","\n","    return codebook_mapping, codebook_indices, q_loss\n","\n","  def decode(self, z):\n","    post_quant_conv_mapping = self.post_quant_conv(z)\n","    decoded_images = self.decoder(post_quant_conv_mapping)\n","\n","    return decoded_images\n","\n","  def calculate_lambda(self, perceptual_loss, gan_loss):\n","    last_layer = self.decoder.model[-1]\n","    last_layer_weight = last_layer.weight\n","\n","    perceptual_loss_grads = torch.autograd.grad(perceptual_loss, last_layer_weight, retain_graph=True)[0]\n","    gan_loss_grads = torch.autograd.grad(gan_loss, last_layer_weight, retain_graph=True)[0]\n","\n","    l = torch.norm(perceptual_loss_grads)/(torch.norm(gan_loss_grads)-1e-4)\n","    l = torch.clamp(l, 0, 1e-4).detach()\n","\n","    return 0.8 * l\n","\n","  @staticmethod\n","  def adopt_weight(disc_factor, i, threshold, value=0.):\n","      if i < threshold:\n","          disc_factor = value\n","      return disc_factor\n","\n","\n","  def load_checkpoint(self, path):\n","    self.load_state_dict(torch.load(path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')))"],"metadata":{"id":"j1pkfXgeapVj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Discriminator\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, args, num_filters_last=64, n_layers=3):\n","        super(Discriminator, self).__init__()\n","\n","        layers = [nn.Conv2d(args.image_channels, num_filters_last, 4, 2, 1), nn.LeakyReLU(0.2)]\n","        num_filters_mult = 1\n","\n","        for i in range(1, n_layers + 1):\n","            num_filters_mult_last = num_filters_mult\n","            num_filters_mult = min(2 ** i, 8)\n","            layers += [\n","                nn.Conv2d(num_filters_last * num_filters_mult_last, num_filters_last * num_filters_mult, 4,\n","                          2 if i < n_layers else 1, 1, bias=False),\n","                nn.BatchNorm2d(num_filters_last * num_filters_mult),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        layers.append(nn.Conv2d(num_filters_last * num_filters_mult, 1, 4, 1, 1))\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","      return self.model(x)"],"metadata":{"id":"JLR-ahLXhIwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#LPIP\n","\n","\n","\n","\n","URL_MAP = {\n","    \"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"\n","}\n","\n","CKPT_MAP = {\n","    \"vgg_lpips\": \"vgg.pth\"\n","}\n","\n","\n","def download(url, local_path, chunk_size=1024):\n","    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n","    with requests.get(url, stream=True) as r:\n","        total_size = int(r.headers.get(\"content-length\", 0))\n","        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n","            with open(local_path, \"wb\") as f:\n","                for data in r.iter_content(chunk_size=chunk_size):\n","                    if data:\n","                        f.write(data)\n","                        pbar.update(chunk_size)\n","\n","\n","def get_ckpt_path(name, root):\n","    assert name in URL_MAP\n","    path = os.path.join(root, CKPT_MAP[name])\n","    if not os.path.exists(path):\n","        print(f\"Downloading {name} model from {URL_MAP[name]} to {path}\")\n","        download(URL_MAP[name], path)\n","    return path\n","\n","\n","class LPIPS(nn.Module):\n","    def __init__(self):\n","        super(LPIPS, self).__init__()\n","        self.scaling_layer = ScalingLayer()\n","        self.channels = [64, 128, 256, 512, 512]\n","        self.vgg = VGG16()\n","        self.lins = nn.ModuleList([\n","            NetLinLayer(self.channels[0]),\n","            NetLinLayer(self.channels[1]),\n","            NetLinLayer(self.channels[2]),\n","            NetLinLayer(self.channels[3]),\n","            NetLinLayer(self.channels[4])\n","        ])\n","\n","        self.load_from_pretrained()\n","\n","        for param in self.parameters():\n","            param.requires_grad = False\n","\n","    def load_from_pretrained(self, name=\"vgg_lpips\"):\n","        ckpt = get_ckpt_path(name, \"./drive/MyDrive/VAE_learning\")\n","        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cuda:0\")), strict=False)\n","\n","    def forward(self, real_x, fake_x):\n","        features_real = self.vgg(self.scaling_layer(real_x))\n","        features_fake = self.vgg(self.scaling_layer(fake_x))\n","        diffs = {}\n","\n","        for i in range(len(self.channels)):\n","            diffs[i] = (norm_tensor(features_real[i]) - norm_tensor(features_fake[i])) ** 2\n","\n","        return sum([spatial_average(self.lins[i].model(diffs[i])) for i in range(len(self.channels))])\n","\n","\n","\n","class ScalingLayer(nn.Module):\n","    def __init__(self):\n","        super(ScalingLayer, self).__init__()\n","        self.register_buffer(\"shift\", torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n","        self.register_buffer(\"scale\", torch.Tensor([.458, .448, .450])[None, :, None, None])\n","\n","    def forward(self, x):\n","        return (x - self.shift) / self.scale\n","\n","class NetLinLayer(nn.Module):\n","    def __init__(self, in_channels, out_channels=1):\n","        super(NetLinLayer, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n","        )\n","\n","\n","class VGG16(nn.Module):\n","    def __init__(self):\n","        super(VGG16, self).__init__()\n","        vgg_pretrained_features = vgg16(pretrained=True).features\n","        slices = [vgg_pretrained_features[i] for i in range(30)]\n","        self.slice1 = nn.Sequential(*slices[0:4])\n","        self.slice2 = nn.Sequential(*slices[4:9])\n","        self.slice3 = nn.Sequential(*slices[9:16])\n","        self.slice4 = nn.Sequential(*slices[16:23])\n","        self.slice5 = nn.Sequential(*slices[23:30])\n","\n","        for param in self.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, x):\n","        h = self.slice1(x)\n","        h_relu1 = h\n","        h = self.slice2(h)\n","        h_relu2 = h\n","        h = self.slice3(h)\n","        h_relu3 = h\n","        h = self.slice4(h)\n","        h_relu4 = h\n","        h = self.slice5(h)\n","        h_relu5 = h\n","        vgg_outputs = namedtuple(\"VGGOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n","        return vgg_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n","\n","\n","def norm_tensor(x):\n","    \"\"\"\n","    Normalize images by their length to make them unit vector?\n","    :param x: batch of images\n","    :return: normalized batch of images\n","    \"\"\"\n","    norm_factor = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n","    return x / (norm_factor + 1e-10)\n","\n","\n","def spatial_average(x):\n","    \"\"\"\n","     imgs have: batch_size x channels x width x height --> average over width and height channel\n","    :param x: batch of images\n","    :return: averaged images along width and height\n","    \"\"\"\n","    return x.mean([2, 3], keepdim=True)"],"metadata":{"id":"lLPPiTjShkPU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Utils\n","\n","\n","\n","# --------------------------------------------- #\n","#                  Data Utils\n","# --------------------------------------------- #\n","\n","class ImagePaths(Dataset):\n","    def __init__(self, path, size=None):\n","        self.size = size\n","\n","        layout = BIDSLayout(os.join)\n","\n","        self.images = [os.path.join(path, file) for file in os.listdir(path)]\n","        self._length = len(self.images)\n","\n","        self.rescaler = albumentations.SmallestMaxSize(max_size=self.size)\n","        self.cropper = albumentations.CenterCrop(height=self.size, width=self.size)\n","        self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n","\n","    def __len__(self):\n","        return self._length\n","\n","    def preprocess_image(self, image_path):\n","        image = Image.open(image_path)\n","        if not image.mode == \"RGB\":\n","            image = image.convert(\"RGB\")\n","        image = np.array(image).astype(np.uint8)\n","        image = self.preprocessor(image=image)[\"image\"]\n","        image = (image / 127.5 - 1.0).astype(np.float32)\n","        image = image.transpose(2, 0, 1)\n","        return image\n","\n","    def __getitem__(self, i):\n","        example = self.preprocess_image(self.images[i])\n","        return example\n","\n","\n","def load_data(args):\n","    train_data = ImagePaths(args.dataset_path, size=256)\n","\n","    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n","    return train_loader\n","\n","\n","# --------------------------------------------- #\n","#                  Module Utils\n","#            for Encoder, Decoder etc.\n","# --------------------------------------------- #\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d):\n","        nn.init.normal_(m.weight.data, 0.0, 0.02).to(device = \"cuda:0\")\n","    elif isinstance(m, nn.BatchNorm2d):\n","        nn.init.normal_(m.weight.data, 1.0, 0.02).to(device = \"cuda:0\")\n","        nn.init.constant_(m.bias.data, 0).to(device = \"cuda:0\")\n","\n","\n","def plot_images(images):\n","    x = images[\"input\"]\n","    reconstruction = images[\"rec\"]\n","    half_sample = images[\"half_sample\"]\n","    full_sample = images[\"full_sample\"]\n","\n","    fig, axarr = plt.subplots(1, 4)\n","    axarr[0].imshow(x.cuda().detach().numpy()[0].transpose(1, 2, 0))\n","    axarr[1].imshow(reconstruction.cuda().detach().numpy()[0].transpose(1, 2, 0))\n","    axarr[2].imshow(half_sample.cuda().detach().numpy()[0].transpose(1, 2, 0))\n","    axarr[3].imshow(full_sample.cuda().detach().numpy()[0].transpose(1, 2, 0))\n","    plt.show()"],"metadata":{"id":"pFt_QvS7iAGX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training VQGAN\n","\n","class TrainVQGAN:\n","  def __init__(self, args):\n","    self.work_dir = args.work_dir\n","    self.results_dir = os.path.join(self.work_dir, \"results\")\n","    self.checkpoint_dir = os.path.join(self.work_dir, \"checkpoints\")\n","\n","    self.vqgan = VQGAN(args).to(device=torch.device(args.device))\n","    self.discriminator = Discriminator(args).to(device=torch.device(args.device))\n","    self.discriminator.apply(weights_init)\n","    self.discriminator.to(device=torch.device(args.device))\n","    self.perceptual_loss = LPIPS().eval().to(device=torch.device(args.device))\n","    self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n","\n","    self.prepare_training()\n","\n","    self.train(args)\n","\n","  def configure_optimizers(self, args):\n","    lr = args.learning_rate\n","    opt_vq = torch.optim.Adam(\n","        list(self.vqgan.encoder.parameters()) + \\\n","        list(self.vqgan.decoder.parameters()) + \\\n","        list(self.vqgan.codebook.parameters()) + \\\n","        list(self.vqgan.quant_conv.parameters()) + \\\n","        list(self.vqgan.post_quant_conv.parameters()),\n","        lr=lr, eps = 1e-8, betas=(args.beta1, args.beta2)\n","    )\n","    opt_disc = torch.optim.Adam(self.discriminator.parameters(), lr=lr, eps=1e-8, betas=(args.beta1, args.beta2))\n","\n","    return opt_vq, opt_disc\n","\n","  @staticmethod\n","  def prepare_training():\n","    os.makedirs(os.path.join(work_dir, \"results\"), exist_ok=True)\n","    os.makedirs(os.path.join(work_dir, \"checkpoints\"), exist_ok=True)\n","\n","  def train(self, args):\n","    train_dataset = load_data(args)\n","    steps_per_epoch = len(train_dataset)\n","    for epoch in range(args.epochs):\n","        with tqdm(range(len(train_dataset))) as pbar:\n","            for i, imgs in zip(pbar, train_dataset):\n","                imgs = imgs.to(device=args.device)\n","                decoded_images, _, q_loss = self.vqgan(imgs)\n","\n","                disc_real = self.discriminator(imgs)\n","                disc_fake = self.discriminator(decoded_images)\n","\n","                disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch*steps_per_epoch+i, threshold=args.disc_start)\n","\n","                perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n","                rec_loss = torch.abs(imgs - decoded_images)\n","                perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss + args.rec_loss_factor * rec_loss\n","                perceptual_rec_loss = perceptual_rec_loss.mean()\n","                g_loss = -torch.mean(disc_fake)\n","\n","                Œª = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n","                vq_loss = perceptual_rec_loss + q_loss + disc_factor * Œª * g_loss\n","\n","                d_loss_real = torch.mean(F.relu(1. - disc_real))\n","                d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n","                gan_loss = disc_factor * 0.5*(d_loss_real + d_loss_fake)\n","\n","                self.opt_vq.zero_grad()\n","                vq_loss.backward(retain_graph=True)\n","\n","                self.opt_disc.zero_grad()\n","                gan_loss.backward()\n","\n","                self.opt_vq.step()\n","                self.opt_disc.step()\n","\n","                if i % 400 == 0:\n","                    with torch.no_grad():\n","                        real_fake_images = torch.cat((imgs[:4], decoded_images.add(1).mul(0.5)[:4]))\n","                        vutils.save_image(real_fake_images, os.path.join(self.results_dir, f\"{epoch}_{i}.jpg\"), nrow=4)\n","\n","                pbar.set_postfix(\n","                    VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n","                    GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n","                )\n","                pbar.update(0)\n","            torch.save(self.vqgan.state_dict(), os.path.join(self.checkpoint_dir, f\"vqgan_epoch_{epoch}.pt\"))\n","\n","\n","\n"],"metadata":{"id":"KSOEIvvDjAha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Generator:\n","  def __init__(self, args):\n","    self.work_dir = args.work_dir\n","    self.results_dir = os.path.join(self.work_dir, \"results\")\n","    self.checkpoint_dir = os.path.join(self.work_dir, \"checkpoints\")\n","\n","    self.vqgan = VQGAN(args).to(device=torch.device(args.device))\n","    self.vqgan.load_checkpoint(os.path.join(self.checkpoint_dir, args.checkpoint))\n","    self.generate(args)\n","\n","  @staticmethod\n","  def prepare_gen():\n","    os.makedirs(os.path.join(work_dir, \"results\"), exist_ok=True)\n","\n","  def generate(self, args):\n","    inputs = load_data(args)\n","    with tqdm(range(len(inputs))) as pbar:\n","      for i, img in zip(pbar, inputs):\n","        img = img.to(device=args.device)\n","        result, _, _ = self.vqgan(img)\n","        vutils.save_image(result, os.path.join(self.results_dir, f\"result_{i}.jpg\"), nrow=4)\n","        pbar.update(0)\n"],"metadata":{"id":"tEXFjMmzIIsU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Arguments:\n","  def __init__(self, args):\n","    for k, v in args.items():\n","      setattr(self, k, v)"],"metadata":{"id":"85az58hX2G4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_path = './drive/MyDrive/VAE_learning/anime_faces/cropped/'\n","args_vqgan ={\n","    'mode': \"gen\",\n","    'checkpoint': \"vqgan_epoch_0.pt\",\n","    'latent_dim': 256,\n","    'image_size': 64,\n","    'num_codebook_vectors': 1024,\n","    'beta': 0.25,\n","    'image_channels': 3,\n","    'dataset_path': os.path.join(work_dir, \"inputs\"),\n","    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    'batch_size': 1,\n","    'epochs': 5,\n","    'learning_rate': 2.25e-5,\n","    'beta1': 0.5,\n","    'beta2': 0.9,\n","    'disc_start': 100,\n","    'disc_factor': 1,\n","    'rec_loss_factor': 1,\n","    'perceptual_loss_factor':1,\n","    'work_dir': work_dir\n","}\n","args_vqgan = Arguments(args_vqgan)"],"metadata":{"id":"o3tSefwtUJct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if args_vqgan.mode == 'train':\n","  trainer = TrainVQGAN(args_vqgan)\n","else:\n","  generator = Generator(args_vqgan)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6OdSOWexxjuy","executionInfo":{"status":"ok","timestamp":1698696223832,"user_tz":-60,"elapsed":18711,"user":{"displayName":"Pablo Gonz√°lez Santamarta","userId":"05322282046654241130"}},"outputId":"ffa104b9-b14d-47ce-e290-8cd29011f6fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:16<00:00,  8.45s/it]\n"]}]}]}